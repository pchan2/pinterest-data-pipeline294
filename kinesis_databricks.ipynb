{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mounting S3 bucket to Databricks\n",
    "\n",
    "- Mount creates a link between a workspace and cloud object storage, which enables you to interact <br /> \n",
    "with cloud object storage using familiar file paths relative to the Databricks file system.\n",
    "- To open a new notebook: `New > Notebook`\n",
    "- Ref: [Databricks notebook](https://dbc-b54c5c54-233d.cloud.databricks.com/?o=1865928197306450#notebook/3155158148749029)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should see the CSV files you uploaded earlier is now inside the\n",
    "# FileStore tables folder.\n",
    "dbutils.fs.ls('/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark functions.\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing.\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file to spark dataframe.\n",
    "aws_keys_df = spark.read.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('sep', ',') \\\n",
    "    .load('/FileStore/tables/authentication_credentials.csv')\n",
    "\n",
    "aws_keys_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe.\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user') \\\n",
    "                        .select('Access key ID') \\\n",
    "                        .collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user') \\\n",
    "                        .select('Secret access key') \\\n",
    "                        .collect()[0]['Secret access key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read streaming data from Kinesis\n",
    "- [Medium: stream-data-from-kinesis-to-databricks-with-pyspark](https://medium.com/road-to-data-engineering/stream-data-from-kinesis-to-databricks-with-pyspark-813c516b4233)\n",
    "- [Learn how to process Steaming Data with DataBricks and Amazon Kinesis [ hands on Demo ]](https://www.youtube.com/watch?v=2s08mk6vfDk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIN_STREAM_NAME = \"streaming-0a966c04ad33-pin\"\n",
    "\n",
    "kinesis_df_pin = spark \\\n",
    "                    .readStream \\\n",
    "                    .format('kinesis') \\\n",
    "                    .option('streamName', PIN_STREAM_NAME) \\\n",
    "                    .option('initialPosition', 'earliest') \\\n",
    "                    .option('format', 'json') \\\n",
    "                    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "                    .option('awsSecretKey', SECRET_KEY) \\\n",
    "                    .option('inferSchema', 'true') \\\n",
    "                    .load()\n",
    "\n",
    "display(kinesis_df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEO_STREAM_NAME = \"streaming-0a966c04ad33-geo\"\n",
    "\n",
    "kinesis_df_geo = spark \\\n",
    "                    .readStream \\\n",
    "                    .format('kinesis') \\\n",
    "                    .option('streamName', GEO_STREAM_NAME) \\\n",
    "                    .option('initialPosition', 'earliest') \\\n",
    "                    .option('format', 'json') \\\n",
    "                    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "                    .option('awsSecretKey', SECRET_KEY) \\\n",
    "                    .option('inferSchema', 'true') \\\n",
    "                    .load()\n",
    "\n",
    "display(kinesis_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_STREAM_NAME = \"streaming-0a966c04ad33-user\"\n",
    "\n",
    "kinesis_df_user = spark \\\n",
    "                    .readStream \\\n",
    "                    .format('kinesis') \\\n",
    "                    .option('streamName', USER_STREAM_NAME) \\\n",
    "                    .option('initialPosition', 'earliest') \\\n",
    "                    .option('format', 'json') \\\n",
    "                    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "                    .option('awsSecretKey', SECRET_KEY) \\\n",
    "                    .option('inferSchema', 'true') \\\n",
    "                    .load()\n",
    "\n",
    "display(kinesis_df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the schema for the binary data (payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "pin_schema = StructType() \\\n",
    "                .add('category', StringType()) \\\n",
    "                .add('description', StringType()) \\\n",
    "                .add('downloaded', LongType()) \\\n",
    "                .add('follower_count', StringType()) \\\n",
    "                .add('image_src', StringType()) \\\n",
    "                .add('index', LongType()) \\\n",
    "                .add('is_image_or_video', StringType()) \\\n",
    "                .add('poster_name', StringType()) \\\n",
    "                .add('save_location', StringType()) \\\n",
    "                .add('tag_list', StringType()) \\\n",
    "                .add('title', StringType()) \\\n",
    "                .add('unique_id', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_timestamp_schema = StructType() \\\n",
    "                        .add('$date', StringType())\n",
    "\n",
    "geo_schema = StructType() \\\n",
    "                .add('country', StringType()) \\\n",
    "                .add('ind', LongType()) \\\n",
    "                .add('latitude', DoubleType()) \\\n",
    "                .add('longitude', DoubleType()) \\\n",
    "                .add('timestamp', geo_timestamp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_date_joined_schema = StructType() \\\n",
    "                            .add('$date', StringType())\n",
    "\n",
    "user_schema = StructType() \\\n",
    "                .add('age', LongType()) \\\n",
    "                .add('date_joined', user_date_joined_schema) \\\n",
    "                .add('first_name', StringType()) \\\n",
    "                .add('ind', LongType()) \\\n",
    "                .add('last_name', StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data from the payload and use transformation to do your analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pin = kinesis_df_pin \\\n",
    "          .selectExpr('cast (data as STRING) jsonData') \\\n",
    "          .select(from_json('jsonData', pin_schema).alias('pin')) \\\n",
    "          .select('pin.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = kinesis_df_geo \\\n",
    "          .selectExpr('cast (data as STRING) jsonData') \\\n",
    "          .select(from_json('jsonData', geo_schema).alias('geo')) \\\n",
    "          .select('geo.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = kinesis_df_user \\\n",
    "          .selectExpr('cast (data as STRING) jsonData') \\\n",
    "          .select(from_json('jsonData', user_schema).alias('user')) \\\n",
    "          .select('user.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Clean the Pinterest post DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pin)\n",
    "df_pin.printSchema()\n",
    "df_pin.dtypes\n",
    "transformed_df_pin = df_pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty entries and entries with no relevant data in each column\n",
    "# with Nones.\n",
    "# https://www.databricks.com/blog/2017/08/09/apache-sparks-structured-streaming-with-amazon-kinesis-on-databricks.html\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "# Define conditions for updating each column.\n",
    "update_conditions = {\n",
    "    'description':\n",
    "        (col('description') == 'No description available Story format', None),\n",
    "    'follower_count': (col('follower_count') == 'User Info Error', None),\n",
    "    'image_src': (col('image_src') == 'Image src error.', None),\n",
    "    'is_image_or_video':\n",
    "        (~col('is_image_or_video')\n",
    "         .isin(['image', 'video', 'multi-video(story page format)']), None),\n",
    "    'poster_name': (col('poster_name') == 'User Info Error', None),\n",
    "    'tag_list': (col('tag_list') == 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e', None),\n",
    "    'title': (col('title') == 'No Title Data Available', None),\n",
    "}\n",
    "\n",
    "# Apply conditional transformations to update multiple columns.\n",
    "for column, condition in update_conditions.items():\n",
    "    print(condition)\n",
    "    transformed_df_pin = transformed_df_pin \\\n",
    "                            .withColumn(column, when(condition[0], condition[1])\n",
    "                            .otherwise(col(column)))\n",
    "\n",
    "display(transformed_df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates.\n",
    "transformed_df_pin = transformed_df_pin.dropDuplicates()\n",
    "display(transformed_df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary transformations on the follower_count to ensure \n",
    "# every entry is a number. Make sure the data type of this column is an\n",
    "# int.\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "# Pre-transformation of follower_count.\n",
    "display(df_pin.select('follower_count').distinct())\n",
    "\n",
    "transformed_df_pin = transformed_df_pin.withColumn(\n",
    "    'follower_count',\n",
    "    expr('CASE WHEN substring(follower_count, -1) = \"k\" \\\n",
    "               THEN concat(substring(follower_count, 1, \\\n",
    "                    length(follower_count) - 1), \"000\") \\\n",
    "               WHEN substring(follower_count, -1) = \"M\" \\\n",
    "               THEN concat(substring(follower_count, 1, \\\n",
    "                    length(follower_count) - 1), \"000000\") \\\n",
    "               ELSE follower_count \\\n",
    "               END')\n",
    ")\n",
    "\n",
    "# Post-transformation of follower_count.\n",
    "display(transformed_df_pin.select('follower_count').distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert follower_count to int type.\n",
    "transformed_df_pin = transformed_df_pin \\\n",
    "    .withColumn(\"follower_count\",col('follower_count').cast('int'))\n",
    "transformed_df_pin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure columns with numbers are of the numeric type:\n",
    "# downloaded, follower_count, index.\n",
    "transformed_df_pin.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data in the save_location column to include only the save \n",
    "# location path.\n",
    "transformed_df_pin = transformed_df_pin.withColumn( \\\n",
    "    'new_save_location', transformed_df_pin.save_location.substr(14, 30))\n",
    "transformed_df_pin = transformed_df_pin.drop('save_location')\n",
    "transformed_df_pin = transformed_df_pin \\\n",
    "    .withColumnRenamed('new_save_location', 'save_location')\n",
    "display(transformed_df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column.\n",
    "transformed_df_pin = transformed_df_pin.withColumnRenamed('index', 'ind')\n",
    "transformed_df_pin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns.\n",
    "transformed_df_pin = transformed_df_pin.select(\n",
    "    'ind', 'unique_id', 'title', 'description', 'follower_count',\n",
    "    'poster_name', 'tag_list', 'is_image_or_video', 'image_src', \n",
    "    'save_location', 'category')\n",
    "transformed_df_pin.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Clean the geolocation DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo.printSchema()\n",
    "transformed_df_geo = df_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates.\n",
    "transformed_df_geo = transformed_df_geo.dropDuplicates()\n",
    "display(transformed_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column coordinates that contains an array based on the\n",
    "# latitude and longitude columns.\n",
    "transformed_df_geo = transformed_df_geo.withColumn(\n",
    "                        'coordinates', array('latitude', 'longitude'))\n",
    "display(transformed_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the latitude and longitude columns from the DataFrame.\n",
    "# OPTIONAL because later on, the columns will be rearranged to exclude \n",
    "# the columns in question.\n",
    "# https://stackoverflow.com/questions/29600673/how-to-delete-columns-in-pyspark-dataframe\n",
    "# The * is to unpack / destructure the array.\n",
    "columns_to_drop = ['latitude', 'longitude']\n",
    "transformed_df_geo = transformed_df_geo.drop(*columns_to_drop)\n",
    "display(transformed_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamp column from a string to a timestamp data type.\n",
    "transformed_df_geo = transformed_df_geo.withColumn(\n",
    "                        'timestamp', to_timestamp('timestamp.$date')) \n",
    "display(transformed_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns.\n",
    "transformed_df_geo = transformed_df_geo.select(\n",
    "                        'ind', 'country', 'coordinates', 'timestamp')\n",
    "transformed_df_geo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Clean the user DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user.printSchema()\n",
    "transformed_df_user = df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates.\n",
    "transformed_df_user = transformed_df_user.dropDuplicates()\n",
    "display(transformed_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column user_name that concatenates the information found \n",
    "# in the first_name and last_name columns.\n",
    "transformed_df_user = transformed_df_user.withColumn(\n",
    "                        'user_name', concat('first_name', 'last_name'))\n",
    "display(transformed_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first_name and last_name columns from the DataFrame.\n",
    "# OPTIONAL because later on, the columns will be rearranged to exclude \n",
    "# the columns in question.\n",
    "columns_to_drop = ['first_name', 'last_name']\n",
    "transformed_df_user = transformed_df_user.drop(*columns_to_drop)\n",
    "display(transformed_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date_joined column from a string to a timestamp data type.\n",
    "transformed_df_user = transformed_df_user.withColumn(\n",
    "                        'date_joined', to_timestamp('date_joined.$date')) \n",
    "display(transformed_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns.\n",
    "transformed_df_user = transformed_df_user.select(\n",
    "                        'ind', 'user_name', 'age', 'date_joined')\n",
    "transformed_df_user.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Write the streaming data to Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount creates a link between a workspace and cloud object storage, \n",
    "# which enables you to interact with cloud object storage using familiar \n",
    "# file paths relative to the Databricks file system.\n",
    "\n",
    "# Encode the secrete key.\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "# AWS S3 bucket name.\n",
    "AWS_S3_BUCKET = \"user-0a966c04ad33-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/0a966c04ad33-mount\"\n",
    "# Source url.\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive.\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME) # CAN ONLY DO THIS ONCE\n",
    "# To unmount, run: dbutils.fs.unmount(MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to delta tables.\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-partitionby-example/#google_vignette\n",
    "# https://www.youtube.com/watch?v=-OQGEc09xbY\n",
    "\n",
    "transformed_df_pin \\\n",
    "  .writeStream \\\n",
    "  .partitionBy('category') \\\n",
    "  .format('delta') \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option('checkpointLocation',\n",
    "          f'{MOUNT_NAME}/delta/0a966c04ad33_pin_table/_checkpoint') \\\n",
    "  .start(f'{MOUNT_NAME}/delta/0a966c04ad33_pin_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df_geo \\\n",
    "  .writeStream \\\n",
    "  .partitionBy('country') \\\n",
    "  .format('delta') \\\n",
    "  .option('checkpointLocation', \n",
    "          f'{MOUNT_NAME}/delta/0a966c04ad33_geo_table/_checkpoint') \\\n",
    "  .start(f'{MOUNT_NAME}/delta/0a966c04ad33_geo_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df_user \\\n",
    "  .writeStream \\\n",
    "  .partitionBy('age') \\\n",
    "  .format('delta') \\\n",
    "  .option('checkpointLocation',\n",
    "          f'{MOUNT_NAME}/delta/0a966c04ad33_user_table/_checkpoint') \\\n",
    "  .start(f'{MOUNT_NAME}/delta/0a966c04ad33_user_table')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
