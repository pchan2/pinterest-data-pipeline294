{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mounting S3 bucket to Databricks\n",
    "\n",
    "- Mount creates a link between a workspace and cloud object storage, which enables you to interact \n",
    "with cloud object storage using familiar file paths relative to the Databricks file system.\n",
    "- To open a new notebook: `New > Notebook`\n",
    "- Ref: [Databricks notebook](https://dbc-b54c5c54-233d.cloud.databricks.com/?o=1865928197306450#notebook/627262318697111/command/627262318697120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should see the CSV files you uploaded earlier is now inside the\n",
    "# FileStore tables folder.\n",
    "dbutils.fs.ls('/FileStore/tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark functions.\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing.\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify file type to be csv.\n",
    "file_type = 'csv'\n",
    "# Indicate file has first row as the header.\n",
    "first_row_is_header = 'true'\n",
    "# Indicate file has comma as the delimeter.\n",
    "delimiter = ','\n",
    "# Read the CSV file to spark dataframe.\n",
    "aws_keys_df = spark.read.format(file_type) \\\n",
    "    .option('header', first_row_is_header) \\\n",
    "    .option('sep', delimiter) \\\n",
    "    .load('/FileStore/tables/authentication_credentials.csv')\n",
    "\n",
    "aws_keys_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe.\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user') \\\n",
    "                        .select('Access key ID') \\\n",
    "                        .collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user') \\\n",
    "                        .select('Secret access key') \\\n",
    "                        .collect()[0]['Secret access key']\n",
    "\n",
    "# Encode the secrete key.\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is not in Databricks as Databricks does not have access to.\n",
    "# the local credentials.yaml file.\n",
    "from database_utils import FileReader\n",
    "\n",
    "\n",
    "creds = FileReader.read('credentials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount creates a link between a workspace and cloud object storage,\n",
    "# which enables you to interact with cloud object storage using familiar\n",
    "# file paths relative to the Databricks file system.\n",
    "\n",
    "IAM_USER_NAME = creds['IAM_USER_NAME']\n",
    "\n",
    "# AWS S3 bucket name.\n",
    "AWS_S3_BUCKET = f'user-{IAM_USER_NAME}-bucket'\n",
    "# Mount name for the bucket.\n",
    "MOUNT_NAME = f'/mnt/{IAM_USER_NAME}-mount'\n",
    "# Source url.\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\" \\\n",
    "                .format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "\n",
    "# Mount the drive only once.\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "\n",
    "# Optional: to unmount, at the EOF, run: dbutils.fs.unmount(MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the S3 bucket was mounted succesfully.\n",
    "display(dbutils.fs.ls(f'{MOUNT_NAME}/../..'))\n",
    "display(dbutils.fs.ls(f'{MOUNT_NAME}/..'))\n",
    "display(dbutils.fs.ls(f'{MOUNT_NAME}/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON format dataset from S3 into Databricks.\n",
    "# S3 Filepath to pin topic:\n",
    "# s3://user-<IAM_USER_NAME>-bucket/topics/<IAM_USER_NAME>.pin/partition=0/\n",
    "\n",
    "# File location and type.\n",
    "# Asterisk(*) indicates reading all the content of the specified file \n",
    "# that have the .json extension.\n",
    "file_location = f'{MOUNT_NAME}/topics/{IAM_USER_NAME}.pin/partition=0/*.json'\n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema.\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket.\n",
    "df_pin = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .load(file_location)\n",
    "# Display Spark dataframe to check its content.\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Filepath to geo topic:\n",
    "# s3://user-<IAM_USER_NAME>-bucket/topics/<IAM_USER_NAME>.geo/partition=0/\n",
    "file_location = f'{MOUNT_NAME}/topics/{IAM_USER_NAME}.geo/partition=0/*.json'\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "\n",
    "df_geo = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Filepath to user topic:\n",
    "# s3://user-<IAM_USER_NAME>-bucket/topics/<IAM_USER_NAME>.user/partition=0/\n",
    "file_location = f'{MOUNT_NAME}/topics/{IAM_USER_NAME}.user/partition=0/*.json'\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "\n",
    "df_user = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Clean the Pinterest post DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pin)\n",
    "df_pin.printSchema()\n",
    "df_pin.dtypes\n",
    "df_pin.describe()\n",
    "df_pin.describe().show()\n",
    "transformed_df_pin = df_pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty entries and entries with no relevant data in each column\n",
    "# with Nones.\n",
    "# https://www.projectpro.io/recipes/explain-fillna-and-fill-functions-pyspark-databricks\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-loop-iterate-through-rows-in-dataframe/\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "# Define conditions for updating each column.\n",
    "update_conditions = {\n",
    "    'description':\n",
    "        (col('description') == 'No description available Story format', None),\n",
    "    'follower_count': (col('follower_count') == 'User Info Error', None),\n",
    "    'image_src': (col('image_src') == 'Image src error.', None),\n",
    "    'is_image_or_video':\n",
    "        (~col('is_image_or_video')\n",
    "         .isin(['image', 'video', 'multi-video(story page format)']), None),\n",
    "    'poster_name': (col('poster_name') == 'User Info Error', None),\n",
    "    'tag_list': (col('tag_list') == 'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e', None),\n",
    "    'title': (col('title') == 'No Title Data Available', None),\n",
    "}\n",
    "\n",
    "# Apply conditional transformations to update multiple columns.\n",
    "for column, condition in update_conditions.items():\n",
    "    print(condition)\n",
    "    transformed_df_pin = transformed_df_pin \\\n",
    "                            .withColumn(column, when(condition[0], condition[1])\n",
    "                            .otherwise(col(column)))\n",
    "\n",
    "display(transformed_df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates.\n",
    "transformed_df_pin = transformed_df_pin.dropDuplicates()\n",
    "display(transformed_df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary transformations on the follower_count to ensure \n",
    "# every entry is a number. Make sure the data type of this column is an\n",
    "# int.\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "# Pre-transformation of follower_count.\n",
    "display(df_pin.select('follower_count').distinct())\n",
    "\n",
    "transformed_df_pin = transformed_df_pin.withColumn(\n",
    "    'follower_count',\n",
    "    expr('CASE WHEN substring(follower_count, -1) = \"k\" \\\n",
    "               THEN concat(substring(follower_count, 1, \\\n",
    "                    length(follower_count) - 1), \"000\") \\\n",
    "               WHEN substring(follower_count, -1) = \"M\" \\\n",
    "               THEN concat(substring(follower_count, 1, \\\n",
    "                    length(follower_count) - 1), \"000000\") \\\n",
    "               ELSE follower_count \\\n",
    "               END')\n",
    ")\n",
    "\n",
    "# Post-transformation of follower_count.\n",
    "display(transformed_df_pin.select('follower_count').distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert follower_count to int type.\n",
    "transformed_df_pin = transformed_df_pin \\\n",
    "    .withColumn('follower_count',col('follower_count').cast('int'))\n",
    "transformed_df_pin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure columns with numbers are of the numeric type:\n",
    "# downloaded, follower_count, index.\n",
    "transformed_df_pin.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data in the save_location column to include only the save \n",
    "# location path.\n",
    "transformed_df_pin = transformed_df_pin.withColumn( \\\n",
    "    'new_save_location', transformed_df_pin.save_location.substr(14, 30))\n",
    "transformed_df_pin = transformed_df_pin.drop('save_location')\n",
    "transformed_df_pin = transformed_df_pin \\\n",
    "    .withColumnRenamed('new_save_location', 'save_location')\n",
    "display(transformed_df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column.\n",
    "transformed_df_pin = transformed_df_pin.withColumnRenamed('index', 'ind')\n",
    "transformed_df_pin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns.\n",
    "transformed_df_pin = transformed_df_pin.select(\n",
    "    'ind', 'unique_id', 'title', 'description', 'follower_count',\n",
    "    'poster_name', 'tag_list', 'is_image_or_video', 'image_src', \n",
    "    'save_location', 'category')\n",
    "transformed_df_pin.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Clean the geolocation DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo.printSchema()\n",
    "transformed_df_geo = df_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates.\n",
    "transformed_df_geo = transformed_df_geo.dropDuplicates()\n",
    "display(transformed_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column coordinates that contains an array based on the\n",
    "# latitude and longitude columns.\n",
    "transformed_df_geo = transformed_df_geo.withColumn(\n",
    "                        'coordinates', array('latitude', 'longitude'))\n",
    "display(transformed_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the latitude and longitude columns from the DataFrame.\n",
    "# OPTIONAL because later on, the columns will be rearranged to exclude \n",
    "# the columns in question.\n",
    "# https://stackoverflow.com/questions/29600673/how-to-delete-columns-in-pyspark-dataframe\n",
    "# The * is to unpack / destructure the array.\n",
    "columns_to_drop = ['latitude', 'longitude']\n",
    "transformed_df_geo = transformed_df_geo.drop(*columns_to_drop)\n",
    "display(transformed_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamp column from a string to a timestamp data type.\n",
    "transformed_df_geo = transformed_df_geo.withColumn(\n",
    "                        'timestamp', to_timestamp('timestamp.$date')) \n",
    "display(transformed_df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns.\n",
    "transformed_df_geo = transformed_df_geo.select(\n",
    "                        'ind', 'country', 'coordinates', 'timestamp')\n",
    "transformed_df_geo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Clean the user DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user.printSchema()\n",
    "transformed_df_user = df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates.\n",
    "transformed_df_user = transformed_df_user.dropDuplicates()\n",
    "display(transformed_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column user_name that concatenates the information found \n",
    "# in the first_name and last_name columns.\n",
    "transformed_df_user = transformed_df_user.withColumn(\n",
    "                        'user_name', concat('first_name', 'last_name'))\n",
    "display(transformed_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first_name and last_name columns from the DataFrame.\n",
    "# OPTIONAL because later on, the columns will be rearranged to exclude \n",
    "# the columns in question.\n",
    "columns_to_drop = ['first_name', 'last_name']\n",
    "transformed_df_user = transformed_df_user.drop(*columns_to_drop)\n",
    "display(transformed_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date_joined column from a string to a timestamp data type.\n",
    "transformed_df_user = transformed_df_user.withColumn(\n",
    "                        'date_joined', to_timestamp('date_joined.$date')) \n",
    "display(transformed_df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns.\n",
    "transformed_df_user = transformed_df_user.select(\n",
    "                        'ind', 'user_name', 'age', 'date_joined')\n",
    "transformed_df_user.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "# Find the most popular Pinterest category people post to based on their \n",
    "# country.\n",
    "# Your query should return a DataFrame that contains the following\n",
    "# columns:\n",
    "#   country\n",
    "#   category\n",
    "#   category_count, a new column containing the desired query output\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-retrieve-top-n-from-each-group-of-dataframe/\n",
    "\n",
    "df_pin_geo = transformed_df_pin \\\n",
    "                .join(transformed_df_geo, \n",
    "                      transformed_df_pin.ind == transformed_df_geo.ind) \\\n",
    "                .select(transformed_df_geo.country,\n",
    "                        transformed_df_pin.category)\n",
    "\n",
    "df_category_count = df_pin_geo.groupby('country', 'category').count() \\\n",
    "                        .withColumnRenamed('count','category_count') \\\n",
    "                        .sort('country', 'category_count', 'category',\n",
    "                              ascending=[True, False, True])\n",
    "\n",
    "display(df_category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 continued.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "\n",
    "window_country = Window.partitionBy('country') \\\n",
    "                    .orderBy(col('category_count').desc())\n",
    "\n",
    "df_category_count = df_category_count \\\n",
    "                        .withColumn('row', row_number().over(window_country)) \\\n",
    "                        .filter(col('row') == 1) \\\n",
    "                        .drop('row')\n",
    "\n",
    "display(df_category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Find which was te most popular category each year.\n",
    "# Find how many posts each category had between 2018 and 2022.\n",
    "# Your query should return a DataFrame that contains the following\n",
    "# columns:\n",
    "#     post_year, a new column that contains only the year from the\n",
    "#       timestamp column\n",
    "#     category\n",
    "#     category_count, a new column containing the desired query output\n",
    "\n",
    "df_pin_geo = transformed_df_pin \\\n",
    "                .join(transformed_df_geo,\n",
    "                      transformed_df_pin.ind == transformed_df_geo.ind) \\\n",
    "                .select(transformed_df_geo.timestamp,\n",
    "                        transformed_df_pin.category)\n",
    "\n",
    "df_pin_geo = df_pin_geo \\\n",
    "                .withColumn('post_year', year(df_pin_geo.timestamp)) \\\n",
    "                .filter((col('post_year') >= 2018) & \n",
    "                        (col('post_year') <= 2022)) \\\n",
    "                .drop('timestamp')\n",
    "\n",
    "df_category_count_2018_2022 = df_pin_geo \\\n",
    "                                .groupby('post_year', 'category').count() \\\n",
    "                                .withColumnRenamed('count','category_count') \\\n",
    "                                .sort('post_year', 'category_count', 'category',\n",
    "                                      ascending=[True, False, True])\n",
    "\n",
    "display(df_category_count_2018_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5 continued.\n",
    "window_year = Window.partitionBy('post_year') \\\n",
    "                    .orderBy(col('category_count').desc())\n",
    "\n",
    "df_category_count_2018_2022 = df_category_count_2018_2022 \\\n",
    "                        .withColumn('row', row_number().over(window_year)) \\\n",
    "                        .filter(col('row') == 1) \\\n",
    "                        .drop('row')\n",
    "\n",
    "display(df_category_count_2018_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Find the user with the most followers in each country.\n",
    "# Step 1: For each country find the user with the most followers.\n",
    "# Your query should return a DataFrame that contains the following columns:\n",
    "#     country\n",
    "#     poster_name\n",
    "#     follower_count\n",
    "\n",
    "df_pin_geo = transformed_df_pin \\\n",
    "                .join(transformed_df_geo, \n",
    "                      transformed_df_pin.ind == transformed_df_geo.ind) \\\n",
    "                .select(transformed_df_geo.country, \n",
    "                        transformed_df_pin.poster_name,\n",
    "                        transformed_df_pin.follower_count)\n",
    "\n",
    "df_follower_count = df_pin_geo.dropDuplicates()\n",
    "df_follower_count = df_follower_count \\\n",
    "                        .sort('country', 'follower_count', 'poster_name', \n",
    "                              ascending=[True, False, True])\n",
    "\n",
    "window_country = Window.partitionBy('country') \\\n",
    "                    .orderBy(col('follower_count').desc())\n",
    "df_follower_count = df_follower_count \\\n",
    "                        .withColumn('row', row_number().over(window_country)) \\\n",
    "                        .filter(col('row') == 1) \\\n",
    "                        .drop('row')\n",
    "\n",
    "display(df_follower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6 continued.\n",
    "# Step 2: Based on the above query, find the country with the user with \n",
    "# most followers.\n",
    "# Your query should return a DataFrame that contains the following \n",
    "# columns:\n",
    "#     country\n",
    "#     follower_count\n",
    "# This DataFrame should have only one entry.\n",
    "\n",
    "window = Window.orderBy(col('follower_count').desc())\n",
    "df_max_follower_count = df_follower_count \\\n",
    "                            .withColumn('row', row_number().over(window)) \\\n",
    "                            .filter(col('row') == 1) \\\n",
    "                            .select('country', 'follower_count')\n",
    "\n",
    "# df_max_follower_count = df_follower_count.agg(max('follower_count'))\n",
    "display(df_max_follower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Find the most popular category for different age groups.\n",
    "# What is the most popular category people post to based on the \n",
    "# following age groups:\n",
    "#     18-24\n",
    "#     25-35\n",
    "#     36-50\n",
    "#     +50\n",
    "\n",
    "# Your query should return a DataFrame that contains the following\n",
    "# columns:\n",
    "#     age_group, a new column based on the original age column\n",
    "#     category\n",
    "#     category_count, a new column containing the desired query output\n",
    "\n",
    "df_pin_user = transformed_df_pin \\\n",
    "                .join(transformed_df_user, \n",
    "                      transformed_df_pin.ind == transformed_df_user.ind) \\\n",
    "                .select(transformed_df_user.age, transformed_df_pin.category)\n",
    "\n",
    "df_category_count_by_age = df_pin_user.withColumn(\n",
    "                'age_group',\n",
    "                expr('CASE WHEN age BETWEEN 18 AND 24 THEN \"18-24\" \\\n",
    "                        WHEN age BETWEEN 25 AND 35 THEN \"25-35\" \\\n",
    "                        WHEN age BETWEEN 36 AND 50 THEN \"36-50\" \\\n",
    "                        WHEN age > 50 THEN \"50+\" \\\n",
    "                        END')\n",
    "            ).drop('age')\n",
    "\n",
    "df_category_count_by_age = df_category_count_by_age \\\n",
    "                            .groupBy('age_group', 'category').count() \\\n",
    "                            .withColumnRenamed('count', 'category_count') \\\n",
    "                            .sort('age_group', 'category_count', 'category', \n",
    "                                  ascending=[True, False, True])\n",
    "\n",
    "window_age = Window.partitionBy('age_group') \\\n",
    "                .orderBy(col('category_count').desc())\n",
    "df_category_count_by_age = df_category_count_by_age \\\n",
    "                            .withColumn('row', row_number().over(window_age)) \\\n",
    "                            .filter(col('row') == 1) \\\n",
    "                            .drop('row')\n",
    "\n",
    "df_category_count_by_age = df_category_count_by_age \\\n",
    "                            .withColumn(\n",
    "                                'age_group', \n",
    "                                when(col('age_group') == '50+', '+50')\n",
    "                                    .otherwise(col('age_group')))\n",
    "\n",
    "display(df_category_count_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8\n",
    "# What is the median follower count for users in the following age groups:\n",
    "#     18-24\n",
    "#     25-35\n",
    "#     36-50\n",
    "#     +50\n",
    "\n",
    "# Your query should return a DataFrame that contains the following columns:\n",
    "#     age_group, a new column based on the original age column\n",
    "#     median_follower_count, a new column containing the desired query output\n",
    "\n",
    "# https://www.educba.com/pyspark-median/\n",
    "# https://www.machinelearningplus.com/pyspark/pyspark-statistics-median/?utm_content=cmp-true\n",
    "\n",
    "df_pin_user = transformed_df_pin \\\n",
    "                .join(transformed_df_user,\n",
    "                      transformed_df_pin.ind == transformed_df_user.ind) \\\n",
    "                .select(transformed_df_user.age,\n",
    "                        transformed_df_pin.follower_count)\n",
    "\n",
    "df_median_follower_count_by_age = df_pin_user \\\n",
    "    .withColumn(\n",
    "        'age_group',\n",
    "        expr('CASE WHEN age BETWEEN 18 AND 24 THEN \"18-24\" \\\n",
    "                WHEN age BETWEEN 25 AND 35 THEN \"25-35\" \\\n",
    "                WHEN age BETWEEN 36 AND 50 THEN \"36-50\" \\\n",
    "                WHEN age > 50 THEN \"50+\" \\\n",
    "                END')\n",
    "    ).select('age_group', 'follower_count') \\\n",
    "        .sort('age_group', 'follower_count')\n",
    "\n",
    "df_median_follower_count_by_age = df_median_follower_count_by_age \\\n",
    "                                    .na.drop('any')\n",
    "\n",
    "display(df_median_follower_count_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8 continued.\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "\n",
    "# Calculate the median\n",
    "df_median_follower_count_by_age = df_median_follower_count_by_age \\\n",
    "                                    .groupBy('age_group') \\\n",
    "                                    .agg(percentile_approx('follower_count', \n",
    "                                                           0.5) \\\n",
    "                                    .alias('median_follower_count')) \\\n",
    "                                    .sort('age_group')\n",
    "\n",
    "display(df_median_follower_count_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8 continued\n",
    "# Replace '50+' with '+50'\n",
    "df_median_follower_count_by_age = df_median_follower_count_by_age \\\n",
    "                                    .withColumn('age_group',\n",
    "                                                when(col('age_group') == '50+',\n",
    "                                                     '+50').otherwise(\n",
    "                                                         col('age_group')))\n",
    "\n",
    "display(df_median_follower_count_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9\n",
    "# Find how many users have joined between 2015 and 2020.\n",
    "# Your query should return a DataFrame that contains the following \n",
    "# columns:\n",
    "#     post_year, a new column that contains only the year from the \n",
    "#       timestamp column\n",
    "#     number_users_joined, a new column containing the desired query \n",
    "#       output\n",
    "\n",
    "df_joined_users_by_year = transformed_df_user \\\n",
    "                            .select(year('date_joined').alias('post_year')) \\\n",
    "                            .filter((col('post_year') >= 2015) & \n",
    "                                    (col('post_year') <= 2020)) \\\n",
    "                            .sort('post_year') \\\n",
    "                            .groupBy('post_year') \\\n",
    "                            .count() \\\n",
    "                            .withColumnRenamed('count', 'number_users_joined')\n",
    "\n",
    "display(df_joined_users_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10\n",
    "# Find the median follower count of users have joined between 2015 and \n",
    "# 2020.\n",
    "# Your query should return a DataFrame that contains the following \n",
    "# columns:\n",
    "#     post_year, a new column that contains only the year from the \n",
    "#       timestamp column\n",
    "#     median_follower_count, a new column containing the desired query\n",
    "#       output\n",
    "\n",
    "df_pin_user = transformed_df_pin \\\n",
    "                .join(transformed_df_user,\n",
    "                      transformed_df_pin.ind == transformed_df_user.ind) \\\n",
    "                .select(transformed_df_user.date_joined, \n",
    "                        transformed_df_pin.follower_count)\n",
    "\n",
    "df_median_follower_count_by_year = df_pin_user \\\n",
    "                                    .select(year('date_joined')\n",
    "                                            .alias('post_year'), \n",
    "                                            'follower_count') \\\n",
    "                                    .filter((col('post_year') >= 2015) & \n",
    "                                            (col('post_year') <= 2020)) \\\n",
    "                                    .sort('post_year', 'follower_count') \\\n",
    "                                    .na.drop('any')\n",
    "\n",
    "# Calculate the median\n",
    "df_median_follower_count_by_year = df_median_follower_count_by_year \\\n",
    "                                    .groupBy('post_year') \\\n",
    "                                    .agg(percentile_approx('follower_count', \n",
    "                                                           0.5) \\\n",
    "                                    .alias('median_follower_count')) \\\n",
    "                                    .sort('post_year')\n",
    "\n",
    "display(df_median_follower_count_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 11\n",
    "# Find the median follower count of users that have joined between 2015 \n",
    "# and 2020, based on which age group they are part of.\n",
    "# Your query should return a DataFrame that contains the following \n",
    "# columns:\n",
    "#     age_group, a new column based on the original age column\n",
    "#     post_year, a new column that contains only the year from the \n",
    "#       timestamp column\n",
    "#     median_follower_count, a new column containing the desired query \n",
    "#       output\n",
    "\n",
    "df_pin_user = transformed_df_pin \\\n",
    "                .join(transformed_df_user, \n",
    "                      transformed_df_pin.ind == transformed_df_user.ind) \\\n",
    "                .select(transformed_df_user.age, \n",
    "                        transformed_df_user.date_joined, \n",
    "                        transformed_df_pin.follower_count)\n",
    "\n",
    "df_median_follower_count_by_age_and_year = df_pin_user \\\n",
    "    .withColumn(\n",
    "        'age_group',\n",
    "        expr('CASE WHEN age BETWEEN 18 AND 24 THEN \"18-24\" \\\n",
    "                WHEN age BETWEEN 25 AND 35 THEN \"25-35\" \\\n",
    "                WHEN age BETWEEN 36 AND 50 THEN \"36-50\" \\\n",
    "                WHEN age > 50 THEN \"50+\" \\\n",
    "                END')\n",
    "    ).select('age_group', \n",
    "             year('date_joined').alias('post_year'),\n",
    "             'follower_count') \\\n",
    "        .sort('age_group', 'post_year', 'follower_count') \\\n",
    "        .na.drop('any')\n",
    "\n",
    "# Calculate the median\n",
    "df_median_follower_count_by_age_and_year = \\\n",
    "    df_median_follower_count_by_age_and_year \\\n",
    "        .groupBy('age_group', 'post_year') \\\n",
    "        .agg(percentile_approx('follower_count', 0.5) \\\n",
    "        .alias('median_follower_count')) \\\n",
    "        .sort('age_group', 'post_year')\n",
    "\n",
    "# Replace '50+' with '+50'\n",
    "df_median_follower_count_by_age_and_year = \\\n",
    "    df_median_follower_count_by_age_and_year \\\n",
    "        .withColumn('age_group',\n",
    "                    when(col('age_group') == '50+', '+50')\n",
    "                    .otherwise(col('age_group')))\n",
    "\n",
    "display(df_median_follower_count_by_age_and_year)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
